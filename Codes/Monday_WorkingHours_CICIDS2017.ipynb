{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mau-r2OhIjHz",
        "outputId": "34da742c-1660-4312-a83e-653b06f94a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.15\n",
            "--2022-12-02 07:10:49--  https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-12-02 07:10:50 ERROR 404: Not Found.\n",
            "\n",
            "tar (child): spark-3.3.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "ls: cannot access '/content/spark-3.3.0-bin-hadoop3': No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.8/dist-packages (0.11.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.7.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib-venn) (1.15.0)\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... 124015 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 124020 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.2.2-3.1ubuntu0.7_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.2.2-3.1ubuntu0.7) ...\n",
            "Setting up libarchive-dev:amd64 (3.2.2-3.1ubuntu0.7) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting libarchive\n",
            "  Downloading libarchive-0.4.7.tar.gz (23 kB)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 7.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: libarchive\n",
            "  Building wheel for libarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libarchive: filename=libarchive-0.4.7-py3-none-any.whl size=31648 sha256=b448ad79458b9e37ae7dd4d88a87b18ed90887d2e63dc0e3e5b2544b219612f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/96/fb/b17d6b9adb7c223428b4c77d9e8bc29f40a51d625389b73461\n",
            "Successfully built libarchive\n",
            "Installing collected packages: nose, libarchive\n",
            "Successfully installed libarchive-0.4.7 nose-1.3.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cartopy\n",
            "  Downloading Cartopy-0.21.0.tar.gz (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 4.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/40/0c/b673fb89eadf798654652df3dda4b8229ca747568976b7a919d7d65e656e/Cartopy-0.21.0.tar.gz#sha256=ce1d3a28a132e94c89ac33769a50f81f65634ab2bd40556317e15bd6cad1ce42 (from https://pypi.org/simple/cartopy/) (requires-python:>=3.8). Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp3sqpzm69 Check the logs for full command output.\u001b[0m\n",
            "  Downloading Cartopy-0.20.3.tar.gz (10.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8 MB 11.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/98/a9/0e4000eabadfcff6373c0fec790863b543b919cbfec18aed60d71ba67d5d/Cartopy-0.20.3.tar.gz#sha256=0d60fa2e2fbd77c4d1f6b1f9d3b588966147f07c1b179d2d34570ac1e1b49006 (from https://pypi.org/simple/cartopy/) (requires-python:>=3.7). Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp_gvuz38g Check the logs for full command output.\u001b[0m\n",
            "  Downloading Cartopy-0.20.2.tar.gz (10.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8 MB 45.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f6/55/1e1c737dc9436b320deead73d1c455ddbb74b8b6992081863492f6f6378a/Cartopy-0.20.2.tar.gz#sha256=4d08c198ecaa50a6a6b109d0f14c070e813defc046a83ac5d7ab494f85599e35 (from https://pypi.org/simple/cartopy/) (requires-python:>=3.7). Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmpfltrj2se Check the logs for full command output.\u001b[0m\n",
            "  Downloading Cartopy-0.20.1.tar.gz (10.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8 MB 31.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/fc/59/aa52698e3838f4cd0e7eaa75bd86837e9e0b05041dbdaee3cda2fffced06/Cartopy-0.20.1.tar.gz#sha256=91f87b130e2574547a20cd634498df97d797abd12dcfd0235bc0cdbcec8b05e3 (from https://pypi.org/simple/cartopy/) (requires-python:>=3.7). Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp92mbdfwe Check the logs for full command output.\u001b[0m\n",
            "  Downloading Cartopy-0.20.0.tar.gz (10.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8 MB 31.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/0f/c0/58453b036e79046d211f083880d58dcce787e7e07647ac25dc46c6555099/Cartopy-0.20.0.tar.gz#sha256=eae58aff26806e63cf115b2bce9477cedc4aa9f578c5e477b2c25cfa404f2b7a (from https://pypi.org/simple/cartopy/) (requires-python:>=3.7). Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmpwu57zw9g Check the logs for full command output.\u001b[0m\n",
            "  Downloading Cartopy-0.19.0.post1.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 26.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from cartopy) (1.21.6)\n",
            "Collecting pyshp>=2\n",
            "  Downloading pyshp-2.3.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from cartopy) (1.8.5.post1)\n",
            "Building wheels for collected packages: cartopy\n",
            "  Building wheel for cartopy (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cartopy: filename=Cartopy-0.19.0.post1-cp38-cp38-linux_x86_64.whl size=12659546 sha256=44a24ab0f91fbd24f19d86b309be8f1f263ae4812bcbe86da5b3af60ba446b70\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/7e/a7/f1de106b1da02e78db1fc6ff482fda367f8902856faa85d1c6\n",
            "Successfully built cartopy\n",
            "Installing collected packages: pyshp, cartopy\n",
            "Successfully installed cartopy-0.19.0.post1 pyshp-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 43 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 56.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=289a2324caa29547f4a7562d4bcda73f75cd2070abcc64bfdd29cd04bcc49be1\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xvzf spark-3.3.0-bin-hadoop3.tgz\n",
        "!ls /content/spark-3.3.0-bin-hadoop3\n",
        "!pip install findspark\n",
        "import os\n",
        "os.environ['Spark_Home'] = '/content/spark-3.3.0-bin-hadoop3'\n",
        "import findspark\n",
        "findspark.init\n",
        "from google.colab import drive \n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot\n",
        "!pip install cartopy\n",
        "import cartopy\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('PySpark 3.3 on google colab').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjIEaVGhIaXZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, \\\n",
        "                                      NaiveBayes, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv(\"/content/drive/MyDrive/BigData&IoT/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\", inferSchema=True, header=True, multiLine=True,\n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "metadata": {
        "id": "LufNbtFNboON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset schema:\")\n",
        "dataset.printSchema()"
      ],
      "metadata": {
        "id": "_bJroQ3rbvtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bda9a8b-0f57-4cea-87e8-1de95dde7137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset schema:\n",
            "root\n",
            " |-- Destination Port: integer (nullable = true)\n",
            " |-- Flow Duration: integer (nullable = true)\n",
            " |-- Total Fwd Packets: integer (nullable = true)\n",
            " |-- Total Backward Packets: integer (nullable = true)\n",
            " |-- Total Length of Fwd Packets: integer (nullable = true)\n",
            " |-- Total Length of Bwd Packets: integer (nullable = true)\n",
            " |-- Fwd Packet Length Max: integer (nullable = true)\n",
            " |-- Fwd Packet Length Min: integer (nullable = true)\n",
            " |-- Fwd Packet Length Mean: double (nullable = true)\n",
            " |-- Fwd Packet Length Std: double (nullable = true)\n",
            " |-- Bwd Packet Length Max: integer (nullable = true)\n",
            " |-- Bwd Packet Length Min: integer (nullable = true)\n",
            " |-- Bwd Packet Length Mean: double (nullable = true)\n",
            " |-- Bwd Packet Length Std: double (nullable = true)\n",
            " |-- Flow Bytes/s: double (nullable = true)\n",
            " |-- Flow Packets/s: double (nullable = true)\n",
            " |-- Flow IAT Mean: double (nullable = true)\n",
            " |-- Flow IAT Std: double (nullable = true)\n",
            " |-- Flow IAT Max: integer (nullable = true)\n",
            " |-- Flow IAT Min: integer (nullable = true)\n",
            " |-- Fwd IAT Total: integer (nullable = true)\n",
            " |-- Fwd IAT Mean: double (nullable = true)\n",
            " |-- Fwd IAT Std: double (nullable = true)\n",
            " |-- Fwd IAT Max: integer (nullable = true)\n",
            " |-- Fwd IAT Min: integer (nullable = true)\n",
            " |-- Bwd IAT Total: integer (nullable = true)\n",
            " |-- Bwd IAT Mean: double (nullable = true)\n",
            " |-- Bwd IAT Std: double (nullable = true)\n",
            " |-- Bwd IAT Max: integer (nullable = true)\n",
            " |-- Bwd IAT Min: integer (nullable = true)\n",
            " |-- Fwd PSH Flags: integer (nullable = true)\n",
            " |-- Bwd PSH Flags: integer (nullable = true)\n",
            " |-- Fwd URG Flags: integer (nullable = true)\n",
            " |-- Bwd URG Flags: integer (nullable = true)\n",
            " |-- Fwd Header Length34: integer (nullable = true)\n",
            " |-- Bwd Header Length: integer (nullable = true)\n",
            " |-- Fwd Packets/s: double (nullable = true)\n",
            " |-- Bwd Packets/s: double (nullable = true)\n",
            " |-- Min Packet Length: integer (nullable = true)\n",
            " |-- Max Packet Length: integer (nullable = true)\n",
            " |-- Packet Length Mean: double (nullable = true)\n",
            " |-- Packet Length Std: double (nullable = true)\n",
            " |-- Packet Length Variance: double (nullable = true)\n",
            " |-- FIN Flag Count: integer (nullable = true)\n",
            " |-- SYN Flag Count: integer (nullable = true)\n",
            " |-- RST Flag Count: integer (nullable = true)\n",
            " |-- PSH Flag Count: integer (nullable = true)\n",
            " |-- ACK Flag Count: integer (nullable = true)\n",
            " |-- URG Flag Count: integer (nullable = true)\n",
            " |-- CWE Flag Count: integer (nullable = true)\n",
            " |-- ECE Flag Count: integer (nullable = true)\n",
            " |-- Down/Up Ratio: integer (nullable = true)\n",
            " |-- Average Packet Size: double (nullable = true)\n",
            " |-- Avg Fwd Segment Size: double (nullable = true)\n",
            " |-- Avg Bwd Segment Size: double (nullable = true)\n",
            " |-- Fwd Header Length55: integer (nullable = true)\n",
            " |-- Fwd Avg Bytes/Bulk: integer (nullable = true)\n",
            " |-- Fwd Avg Packets/Bulk: integer (nullable = true)\n",
            " |-- Fwd Avg Bulk Rate: integer (nullable = true)\n",
            " |-- Bwd Avg Bytes/Bulk: integer (nullable = true)\n",
            " |-- Bwd Avg Packets/Bulk: integer (nullable = true)\n",
            " |-- Bwd Avg Bulk Rate: integer (nullable = true)\n",
            " |-- Subflow Fwd Packets: integer (nullable = true)\n",
            " |-- Subflow Fwd Bytes: integer (nullable = true)\n",
            " |-- Subflow Bwd Packets: integer (nullable = true)\n",
            " |-- Subflow Bwd Bytes: integer (nullable = true)\n",
            " |-- Init_Win_bytes_forward: integer (nullable = true)\n",
            " |-- Init_Win_bytes_backward: integer (nullable = true)\n",
            " |-- act_data_pkt_fwd: integer (nullable = true)\n",
            " |-- min_seg_size_forward: integer (nullable = true)\n",
            " |-- Active Mean: double (nullable = true)\n",
            " |-- Active Std: double (nullable = true)\n",
            " |-- Active Max: integer (nullable = true)\n",
            " |-- Active Min: integer (nullable = true)\n",
            " |-- Idle Mean: double (nullable = true)\n",
            " |-- Idle Std: double (nullable = true)\n",
            " |-- Idle Max: integer (nullable = true)\n",
            " |-- Idle Min: integer (nullable = true)\n",
            " |-- Label: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original dataset sizes: {row} samples, {cols} features\".format(row=dataset.count(), cols=len(dataset.columns)))\n",
        "# replace 'REPLACEMENT CHARACTER' Unicode Character (�) u\"\\uFFFD\" in some Label values\n",
        "dataset = dataset.withColumn(\"Label\", regexp_replace(\"Label\", u\"\\uFFFD \", \"\"))\n",
        "dataset.select(\"Label\").groupBy(\"Label\").count().orderBy(\"count\", ascending=False).show()\n",
        "\n",
        "dataset = dataset.where(col('Flow Duration') > 0) \\\n",
        "                 .where(col('Init_Win_bytes_forward') > 0) \\\n",
        "                 .where(col('Init_Win_bytes_backward') > 0) \\\n",
        "                 .where(col('Flow IAT Min') > 0) \\\n",
        "                 .where(col('Fwd IAT Min') > 0) \\\n",
        "                 .where(col('Fwd IAT Max') > 0)\n",
        "\n",
        "print(\"\\nParsed dataset sizes: {row} samples, {cols} features\".format(row=dataset.count(), cols=len(dataset.columns)))\n",
        "dataset.select(\"Label\").groupBy(\"Label\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ1308Lpbt_7",
        "outputId": "56be02ab-4426-486f-e976-93fa77f97438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset sizes: 529918 samples, 79 features\n",
            "+------+------+\n",
            "| Label| count|\n",
            "+------+------+\n",
            "|BENIGN|529918|\n",
            "+------+------+\n",
            "\n",
            "\n",
            "Parsed dataset sizes: 142031 samples, 79 features\n",
            "+------+------+\n",
            "| Label| count|\n",
            "+------+------+\n",
            "|BENIGN|142031|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq4ZCEwIIStt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20824eba-f1e5-4957-b925-bd05ef8bc4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set Count: 106507\n",
            "Test set Count: 35524\n",
            "\n",
            "Models Evaluation:\n",
            "------------------------\n",
            "Logistic Regression\n",
            "accuracy = 1.00\n",
            "weightedPrecision = 1.00\n",
            "weightedRecall = 1.00\n",
            "f1 = 1.00\n",
            "------------------------\n",
            "Decision Tree\n",
            "accuracy = 1.00\n",
            "weightedPrecision = 1.00\n",
            "weightedRecall = 1.00\n",
            "f1 = 1.00\n",
            "------------------------\n",
            "Random Forest\n",
            "accuracy = 1.00\n",
            "weightedPrecision = 1.00\n",
            "weightedRecall = 1.00\n",
            "f1 = 1.00\n",
            "------------------------\n",
            "Naive Bayes Multinomial\n",
            "accuracy = 1.00\n",
            "weightedPrecision = 1.00\n",
            "weightedRecall = 1.00\n",
            "f1 = 1.00\n",
            "------------------------\n",
            "\n",
            "Running time for Spark job 'Network Attacks Classifier CICIDS2017': 208.71 s\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"Network Attacks Classifier CICIDS2017\").master(\"local\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "features = [f for f in dataset.columns if f not in [\"Label\"]]\n",
        "df_assembler = VectorAssembler(inputCols=features, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
        "dataset = df_assembler.transform(dataset)\n",
        "# dataset.printSchema()\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"Label_Idx\").setHandleInvalid(\"skip\").fit(dataset)\n",
        "dataset = label_indexer.transform(dataset)\n",
        "# dataset.printSchema()\n",
        "\n",
        "# dataset.select([\"Label\", \"Label_Idx\"]).distinct().orderBy(\"Label_Idx\").show()\n",
        "label_list = dataset.select([\"Label\", \"Label_Idx\"]).distinct().orderBy(\"Label_Idx\").select(\"Label\").rdd.flatMap(lambda x: x).collect()\n",
        "# print(label_list)\n",
        "\n",
        "dataset = dataset.select([\"features\",\"Label_Idx\"])\n",
        "# dataset.printSchema()\n",
        "\n",
        "train_set, test_set = dataset.randomSplit([0.75, 0.25], seed=2019)\n",
        "print(\"Training set Count: \" + str(train_set.count()))\n",
        "print(\"Test set Count: \" + str(test_set.count()))\n",
        "\n",
        "# Logistic Regression model\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0.8, featuresCol=\"features\",\n",
        "                        labelCol=\"Label_Idx\", family=\"multinomial\")\n",
        "\n",
        "# Decision Tree model\n",
        "dt = DecisionTreeClassifier(labelCol=\"Label_Idx\", featuresCol=\"features\", maxBins=len(features))\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"Label_Idx\", featuresCol=\"features\", numTrees=20, maxBins=len(features))\n",
        "\n",
        "# Naive Bayes Multinomial\n",
        "nb = NaiveBayes(labelCol=\"Label_Idx\", featuresCol=\"features\", smoothing=1.0, modelType=\"multinomial\")\n",
        "\n",
        "classifiers = {\"Logistic Regression\": lr, \"Decision Tree\": dt,\n",
        "               \"Random Forest\": rf, \"Naive Bayes Multinomial\": nb}\n",
        "\n",
        "metrics = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
        "\n",
        "print(\"\\nModels Evaluation:\")\n",
        "print(\"{:-<24}\".format(\"\"))\n",
        "for idx, c in enumerate(classifiers):\n",
        "\tprint(c)\n",
        "\t# fit the model\n",
        "\tmodel = classifiers[c].fit(train_set)\n",
        "\t\n",
        "\t# make predictions\n",
        "\tpredictions = model.transform(test_set)\n",
        "\tpredictions.cache()\n",
        "\t\n",
        "\t# evaluate performance\n",
        "\tevaluator = MulticlassClassificationEvaluator(labelCol=\"Label_Idx\", predictionCol=\"prediction\")\n",
        "\t\n",
        "\tfor m in metrics:\n",
        "\t\tevaluator.setMetricName(m)\n",
        "\t\tmetric = evaluator.evaluate(predictions)\n",
        "\t\tprint(\"{name} = {value:.2f}\".format(name=m, value=metric))\n",
        "\n",
        "\tprint(\"{:-<24}\".format(\"\"))\n",
        "\n",
        "stop = time.time()\n",
        "print(\"\\nRunning time for Spark job '{name}': {time:.2f} s\"\n",
        "      .format(name=spark.conf.get(\"spark.app.name\"), time=(stop-start)))\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
