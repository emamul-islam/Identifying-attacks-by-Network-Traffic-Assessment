{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xvzf spark-3.3.0-bin-hadoop3.tgz\n",
        "!ls /content/spark-3.3.0-bin-hadoop3\n",
        "!pip install findspark\n",
        "import os\n",
        "os.environ['Spark_Home'] = '/content/spark-3.3.0-bin-hadoop3'\n",
        "import findspark\n",
        "findspark.init\n",
        "from google.colab import drive \n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot\n",
        "!pip install cartopy\n",
        "import cartopy\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('PySpark 3.3 on google colab').getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJoaPml6mcFY",
        "outputId": "5c0f612f-7a76-4422-f5a3-a199d8467769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.15\n",
            "--2022-12-02 06:54:40--  https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-12-02 06:54:40 ERROR 404: Not Found.\n",
            "\n",
            "tar (child): spark-3.3.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "ls: cannot access '/content/spark-3.3.0-bin-hadoop3': No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.8/dist-packages (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.8/dist-packages (0.11.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib-venn) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: libarchive in /usr/local/lib/python3.8/dist-packages (0.4.7)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.8/dist-packages (from libarchive) (1.3.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.8/dist-packages (0.19.0.post1)\n",
            "Requirement already satisfied: pyshp>=2 in /usr/local/lib/python3.8/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from cartopy) (1.21.6)\n",
            "Requirement already satisfied: shapely>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from cartopy) (1.8.5.post1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q0kjuzokEwn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, \\\n",
        "                                      NaiveBayes, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv(\"/content/drive/MyDrive/BigData&IoT/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\", inferSchema=True, header=True, multiLine=True,\n",
        "                         ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
      ],
      "metadata": {
        "id": "B2R9yCIGkV6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset schema:\")\n",
        "dataset.printSchema()"
      ],
      "metadata": {
        "id": "qZ1I3xSLkq6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece00c2f-b5bb-4488-e8cd-228afde39fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset schema:\n",
            "root\n",
            " |-- Destination Port: integer (nullable = true)\n",
            " |-- Flow Duration: integer (nullable = true)\n",
            " |-- Total Fwd Packets: integer (nullable = true)\n",
            " |-- Total Backward Packets: integer (nullable = true)\n",
            " |-- Total Length of Fwd Packets: integer (nullable = true)\n",
            " |-- Total Length of Bwd Packets: integer (nullable = true)\n",
            " |-- Fwd Packet Length Max: integer (nullable = true)\n",
            " |-- Fwd Packet Length Min: integer (nullable = true)\n",
            " |-- Fwd Packet Length Mean: double (nullable = true)\n",
            " |-- Fwd Packet Length Std: double (nullable = true)\n",
            " |-- Bwd Packet Length Max: integer (nullable = true)\n",
            " |-- Bwd Packet Length Min: integer (nullable = true)\n",
            " |-- Bwd Packet Length Mean: double (nullable = true)\n",
            " |-- Bwd Packet Length Std: double (nullable = true)\n",
            " |-- Flow Bytes/s: double (nullable = true)\n",
            " |-- Flow Packets/s: double (nullable = true)\n",
            " |-- Flow IAT Mean: double (nullable = true)\n",
            " |-- Flow IAT Std: double (nullable = true)\n",
            " |-- Flow IAT Max: integer (nullable = true)\n",
            " |-- Flow IAT Min: integer (nullable = true)\n",
            " |-- Fwd IAT Total: integer (nullable = true)\n",
            " |-- Fwd IAT Mean: double (nullable = true)\n",
            " |-- Fwd IAT Std: double (nullable = true)\n",
            " |-- Fwd IAT Max: integer (nullable = true)\n",
            " |-- Fwd IAT Min: integer (nullable = true)\n",
            " |-- Bwd IAT Total: integer (nullable = true)\n",
            " |-- Bwd IAT Mean: double (nullable = true)\n",
            " |-- Bwd IAT Std: double (nullable = true)\n",
            " |-- Bwd IAT Max: integer (nullable = true)\n",
            " |-- Bwd IAT Min: integer (nullable = true)\n",
            " |-- Fwd PSH Flags: integer (nullable = true)\n",
            " |-- Bwd PSH Flags: integer (nullable = true)\n",
            " |-- Fwd URG Flags: integer (nullable = true)\n",
            " |-- Bwd URG Flags: integer (nullable = true)\n",
            " |-- Fwd Header Length34: integer (nullable = true)\n",
            " |-- Bwd Header Length: integer (nullable = true)\n",
            " |-- Fwd Packets/s: double (nullable = true)\n",
            " |-- Bwd Packets/s: double (nullable = true)\n",
            " |-- Min Packet Length: integer (nullable = true)\n",
            " |-- Max Packet Length: integer (nullable = true)\n",
            " |-- Packet Length Mean: double (nullable = true)\n",
            " |-- Packet Length Std: double (nullable = true)\n",
            " |-- Packet Length Variance: double (nullable = true)\n",
            " |-- FIN Flag Count: integer (nullable = true)\n",
            " |-- SYN Flag Count: integer (nullable = true)\n",
            " |-- RST Flag Count: integer (nullable = true)\n",
            " |-- PSH Flag Count: integer (nullable = true)\n",
            " |-- ACK Flag Count: integer (nullable = true)\n",
            " |-- URG Flag Count: integer (nullable = true)\n",
            " |-- CWE Flag Count: integer (nullable = true)\n",
            " |-- ECE Flag Count: integer (nullable = true)\n",
            " |-- Down/Up Ratio: integer (nullable = true)\n",
            " |-- Average Packet Size: double (nullable = true)\n",
            " |-- Avg Fwd Segment Size: double (nullable = true)\n",
            " |-- Avg Bwd Segment Size: double (nullable = true)\n",
            " |-- Fwd Header Length55: integer (nullable = true)\n",
            " |-- Fwd Avg Bytes/Bulk: integer (nullable = true)\n",
            " |-- Fwd Avg Packets/Bulk: integer (nullable = true)\n",
            " |-- Fwd Avg Bulk Rate: integer (nullable = true)\n",
            " |-- Bwd Avg Bytes/Bulk: integer (nullable = true)\n",
            " |-- Bwd Avg Packets/Bulk: integer (nullable = true)\n",
            " |-- Bwd Avg Bulk Rate: integer (nullable = true)\n",
            " |-- Subflow Fwd Packets: integer (nullable = true)\n",
            " |-- Subflow Fwd Bytes: integer (nullable = true)\n",
            " |-- Subflow Bwd Packets: integer (nullable = true)\n",
            " |-- Subflow Bwd Bytes: integer (nullable = true)\n",
            " |-- Init_Win_bytes_forward: integer (nullable = true)\n",
            " |-- Init_Win_bytes_backward: integer (nullable = true)\n",
            " |-- act_data_pkt_fwd: integer (nullable = true)\n",
            " |-- min_seg_size_forward: integer (nullable = true)\n",
            " |-- Active Mean: double (nullable = true)\n",
            " |-- Active Std: double (nullable = true)\n",
            " |-- Active Max: integer (nullable = true)\n",
            " |-- Active Min: integer (nullable = true)\n",
            " |-- Idle Mean: double (nullable = true)\n",
            " |-- Idle Std: double (nullable = true)\n",
            " |-- Idle Max: integer (nullable = true)\n",
            " |-- Idle Min: integer (nullable = true)\n",
            " |-- Label: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original dataset sizes: {row} samples, {cols} features\".format(row=dataset.count(), cols=len(dataset.columns)))\n",
        "# replace 'REPLACEMENT CHARACTER' Unicode Character (ï¿½) u\"\\uFFFD\" in some Label values\n",
        "dataset = dataset.withColumn(\"Label\", regexp_replace(\"Label\", u\"\\uFFFD \", \"\"))\n",
        "dataset.select(\"Label\").groupBy(\"Label\").count().orderBy(\"count\", ascending=False).show()\n",
        "\n",
        "dataset = dataset.where(col('Flow Duration') > 0) \\\n",
        "                 .where(col('Init_Win_bytes_forward') > 0) \\\n",
        "                 .where(col('Init_Win_bytes_backward') > 0) \\\n",
        "                 .where(col('Flow IAT Min') > 0) \\\n",
        "                 .where(col('Fwd IAT Min') > 0) \\\n",
        "                 .where(col('Fwd IAT Max') > 0)\n",
        "\n",
        "print(\"\\nParsed dataset sizes: {row} samples, {cols} features\".format(row=dataset.count(), cols=len(dataset.columns)))\n",
        "dataset.select(\"Label\").groupBy(\"Label\").count().orderBy(\"count\", ascending=False).show()"
      ],
      "metadata": {
        "id": "XjEolAfykrj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7c0bb0-d242-4755-efef-35bf48cfe767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset sizes: 692703 samples, 79 features\n",
            "+----------------+------+\n",
            "|           Label| count|\n",
            "+----------------+------+\n",
            "|          BENIGN|440031|\n",
            "|        DoS Hulk|231073|\n",
            "|   DoS GoldenEye| 10293|\n",
            "|   DoS slowloris|  5796|\n",
            "|DoS Slowhttptest|  5499|\n",
            "|      Heartbleed|    11|\n",
            "+----------------+------+\n",
            "\n",
            "\n",
            "Parsed dataset sizes: 214737 samples, 79 features\n",
            "+----------------+------+\n",
            "|           Label| count|\n",
            "+----------------+------+\n",
            "|          BENIGN|113181|\n",
            "|        DoS Hulk| 91035|\n",
            "|   DoS GoldenEye|  7139|\n",
            "|   DoS slowloris|  1823|\n",
            "|DoS Slowhttptest|  1559|\n",
            "+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Network Attacks Classifier CICIDS2017\").master(\"local\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "features = [f for f in dataset.columns if f not in [\"Label\"]]\n",
        "df_assembler = VectorAssembler(inputCols=features, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
        "dataset = df_assembler.transform(dataset)\n",
        "# dataset.printSchema()\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"Label_Idx\").setHandleInvalid(\"skip\").fit(dataset)\n",
        "dataset = label_indexer.transform(dataset)\n",
        "# dataset.printSchema()\n",
        "\n",
        "# dataset.select([\"Label\", \"Label_Idx\"]).distinct().orderBy(\"Label_Idx\").show()\n",
        "label_list = dataset.select([\"Label\", \"Label_Idx\"]).distinct().orderBy(\"Label_Idx\").select(\"Label\").rdd.flatMap(lambda x: x).collect()\n",
        "# print(label_list)\n",
        "\n",
        "dataset = dataset.select([\"features\",\"Label_Idx\"])\n",
        "# dataset.printSchema()\n",
        "\n",
        "train_set, test_set = dataset.randomSplit([0.75, 0.25], seed=2019)\n",
        "print(\"Training set Count: \" + str(train_set.count()))\n",
        "print(\"Test set Count: \" + str(test_set.count()))\n",
        "\n",
        "# Logistic Regression model\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0.8, featuresCol=\"features\",\n",
        "                        labelCol=\"Label_Idx\", family=\"multinomial\")\n",
        "\n",
        "# Decision Tree model\n",
        "dt = DecisionTreeClassifier(labelCol=\"Label_Idx\", featuresCol=\"features\", maxBins=len(features))\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(labelCol=\"Label_Idx\", featuresCol=\"features\", numTrees=20, maxBins=len(features))\n",
        "\n",
        "# Naive Bayes Multinomial\n",
        "nb = NaiveBayes(labelCol=\"Label_Idx\", featuresCol=\"features\", smoothing=1.0, modelType=\"multinomial\")\n",
        "\n",
        "classifiers = {\"Logistic Regression\": lr, \"Decision Tree\": dt,\n",
        "               \"Random Forest\": rf, \"Naive Bayes Multinomial\": nb}\n",
        "\n",
        "metrics = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
        "\n",
        "print(\"\\nModels Evaluation:\")\n",
        "print(\"{:-<24}\".format(\"\"))\n",
        "for idx, c in enumerate(classifiers):\n",
        "\tprint(c)\n",
        "\t# fit the model\n",
        "\tmodel = classifiers[c].fit(train_set)\n",
        "\t\n",
        "\t# make predictions\n",
        "\tpredictions = model.transform(test_set)\n",
        "\tpredictions.cache()\n",
        "\t\n",
        "\t# evaluate performance\n",
        "\tevaluator = MulticlassClassificationEvaluator(labelCol=\"Label_Idx\", predictionCol=\"prediction\")\n",
        "\t\n",
        "\tfor m in metrics:\n",
        "\t\tevaluator.setMetricName(m)\n",
        "\t\tmetric = evaluator.evaluate(predictions)\n",
        "\t\tprint(\"{name} = {value:.2f}\".format(name=m, value=metric))\n",
        "\n",
        "\tprint(\"{:-<24}\".format(\"\"))\n",
        "\n",
        "stop = time.time()\n",
        "print(\"\\nRunning time for Spark job '{name}': {time:.2f} s\"\n",
        "      .format(name=spark.conf.get(\"spark.app.name\"), time=(stop-start)))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1H7dYRM8mGbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10931819-2f0b-44e7-e44d-f77de5313dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set Count: 161091\n",
            "Test set Count: 53646\n",
            "\n",
            "Models Evaluation:\n",
            "------------------------\n",
            "Logistic Regression\n",
            "accuracy = 0.93\n",
            "weightedPrecision = 0.89\n",
            "weightedRecall = 0.93\n",
            "f1 = 0.91\n",
            "------------------------\n",
            "Decision Tree\n",
            "accuracy = 0.99\n",
            "weightedPrecision = 0.99\n",
            "weightedRecall = 0.99\n",
            "f1 = 0.99\n",
            "------------------------\n",
            "Random Forest\n",
            "accuracy = 0.99\n",
            "weightedPrecision = 0.99\n",
            "weightedRecall = 0.99\n",
            "f1 = 0.99\n",
            "------------------------\n",
            "Naive Bayes Multinomial\n",
            "accuracy = 0.61\n",
            "weightedPrecision = 0.84\n",
            "weightedRecall = 0.61\n",
            "f1 = 0.65\n",
            "------------------------\n",
            "\n",
            "Running time for Spark job 'Network Attacks Classifier CICIDS2017': 315.52 s\n"
          ]
        }
      ]
    }
  ]
}